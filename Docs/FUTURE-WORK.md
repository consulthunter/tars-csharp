# Future Work

We have several ideas on how to extend this tool and ideas on how to improve the tool.

## Improvements

Improvements that can/should be made to the tool.

## Extensions

Possible extensions to the tool.

### Test Classification

One idea is to do test classification on tests that are retrieved in the analysis. 

For instance:
- Is it a unit test?
- Is it a integration test?
- Is it a system test?

Figuring out a method to determining the type of method would be particularly beneficial. We see this as being implemented as
another service that could be added to the project model. It should be added during the ```AnalyzeProjectService```

A couple of directions to start exploring:
- Traits for test methods
- Syntax trees
- Namespaces
- Keywords

Likely, all of these will need to be used when determining the type of test.

### Test Generation

Another possibility is adding test generation to the tool.

It would be interesting to add a service that could generate tests for source code.

All the information is already available, syntax trees, compilation, semantic model, even the location of the test methods.

One could add the following:
- Add a service to run the tests of the project being analyzed
- Add a service that generates tests
- Add configuration options for generation
- Add a command for generation

After implementing this, a user could first analyze their projects tests (maybe looking at code coverage and other metrics) then
target methods not being tested. You could use a LLM for the test generation or you could look a more traditional approaches.

Either way, this would be a particularly useful extension to the tool and would make more or less one-of-a-kind.

### Validation

While creating this tool, we wanted to do an experiment to replace tests in projects with those generated by artificial intelligence
like an LLM. However, we couldn't find any datasets for fine-tuning, hence this tool. But we also wanted to validate the datasets and
tool by fine-tuning a model on the dataset and replacing the repositories original tests with those that are generated.

We did implement this but decided against incorporating this into the tool for the following reasons:
- We ran into issues when attempting to resolve dependencies for the repositories where we were replacing tests automatically
- We ran into issues when attempting to run the original tests for the repositories
- Our fine-tuned model produced mostly incorrect tests

Our suggestion is to instead use this tool to create a benchmark of software tests from open-source repositories. 

Then, implement functionality in this tool so that a user can replace tests from the project's in the benchmark.

That way, a user can gather their own dataset, train their model, run the tests from the benchmark, replace tests with generated tests,
and evaluate their dataset/model more or less automatically.